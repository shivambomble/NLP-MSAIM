{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shivam Bomble\n",
    "3MSAIM 2448510 NLP Lab 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from nltk import pos_tag, word_tokenize, ne_chunk\n",
    "from nltk.parse import CoreNLPParser\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. WORDNET OPERATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A. THIRD MEANINGS\n",
      "--------------------------------------------------\n",
      "\n",
      "Word: bear\n",
      "Third meaning: have\n",
      "\n",
      "Word: set\n",
      "Third meaning: several exercises intended to be done in series\n",
      "\n",
      "Word: square\n",
      "Third meaning: an open area at the meeting of two or more streets\n",
      "\n",
      "Word: lead\n",
      "Third meaning: evidence pointing to a possible solution\n",
      "\n",
      "Word: criteria\n",
      "Has fewer than 3 meanings. Available: 2\n"
     ]
    }
   ],
   "source": [
    "# Test words\n",
    "words = ['bear', 'set', 'square', 'lead', 'criteria']\n",
    "\n",
    "# a) Get 3rd meaning\n",
    "print(\"\\nA. THIRD MEANINGS\")\n",
    "print(\"-\" * 50)\n",
    "for word in words:\n",
    "    synsets = wn.synsets(word)\n",
    "    print(f\"\\nWord: {word}\")\n",
    "    if len(synsets) >= 3:\n",
    "        print(f\"Third meaning: {synsets[2].definition()}\")\n",
    "    else:\n",
    "        print(f\"Has fewer than 3 meanings. Available: {len(synsets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "B-E. PARTS OF SPEECH\n",
      "--------------------------------------------------\n",
      "\n",
      "Word: bear\n",
      "Nouns: ['bear']\n",
      "Verbs: ['yield', 'accept', 'hold', 'acquit', 'take_over', 'support', 'put_up', 'carry', 'suffer', 'have', 'endure', 'abide', 'stomach', 'stick_out', 'tolerate', 'give_birth', 'pay', 'birth', 'have_a_bun_in_the_oven', 'conduct', 'deport', 'digest', 'gestate', 'brook', 'behave', 'comport', 'expect', 'turn_out', 'assume', 'stand', 'deliver', 'wear', 'contain', 'bear']\n",
      "Adjectives: []\n",
      "Adverbs: []\n",
      "\n",
      "Word: set\n",
      "Nouns: ['readiness', 'lot', 'band', 'set', 'stage_set', 'Seth', 'circle', 'bent', 'hardening', 'Set', 'exercise_set', 'solidifying', 'solidification', 'curing']\n",
      "Verbs: ['localise', 'congeal', 'do', 'fructify', 'set_up', 'arrange', 'pose', 'coiffure', 'sic', 'adjust', 'gear_up', 'ready', 'plant', 'go_down', 'fix', 'prepare', 'place', 'typeset', 'localize', 'rig', 'coif', 'countersink', 'jell', 'specify', 'position', 'set', 'put', 'limit', 'mark', 'correct', 'coiffe', 'lay_out', 'determine', 'dress', 'lay', 'go_under', 'define']\n",
      "Adjectives: ['dictated', 'laid', 'set', 'placed', 'fixed', 'fit', 'primed', 'determined', 'hardened', 'located', 'situated', 'rigid']\n",
      "Adverbs: []\n",
      "\n",
      "Word: square\n",
      "Nouns: ['foursquare', 'square_toes', 'public_square', 'lame', 'square', 'second_power']\n",
      "Verbs: ['square', 'square_up', 'feather']\n",
      "Adjectives: ['straight', 'hearty', 'substantial', 'satisfying', 'straightforward', 'square', 'solid']\n",
      "Adverbs: ['squarely', 'square']\n",
      "\n",
      "Word: lead\n",
      "Nouns: ['wind', 'tether', 'confidential_information', 'spark_advance', 'hint', 'steer', 'jumper_cable', 'leading', 'lead_story', 'track', 'booster_cable', 'lead', 'atomic_number_82', 'leash', 'trail', 'pencil_lead', 'tip', 'jumper_lead', 'star', 'lead-in', 'principal', 'Pb', 'lede']\n",
      "Verbs: ['contribute', 'moderate', 'precede', 'leave', 'top', 'guide', 'go', 'extend', 'conduct', 'take', 'chair', 'conduce', 'result', 'direct', 'lead', 'pass', 'head', 'run']\n",
      "Adjectives: []\n",
      "Adverbs: []\n",
      "\n",
      "Word: criteria\n",
      "Nouns: ['touchstone', 'criterion', 'measure', 'standard']\n",
      "Verbs: []\n",
      "Adjectives: []\n",
      "Adverbs: []\n"
     ]
    }
   ],
   "source": [
    "# b-e) Get different parts of speech\n",
    "print(\"\\nB-E. PARTS OF SPEECH\")\n",
    "print(\"-\" * 50)\n",
    "for word in words:\n",
    "    synsets = wn.synsets(word)\n",
    "    \n",
    "    # Initialize lists for each POS\n",
    "    nouns = []\n",
    "    verbs = []\n",
    "    adjectives = []\n",
    "    adverbs = []\n",
    "    \n",
    "    # Categorize words\n",
    "    for syn in synsets:\n",
    "        if syn.pos() == 'n':\n",
    "            nouns.extend(lemma.name() for lemma in syn.lemmas())\n",
    "        elif syn.pos() == 'v':\n",
    "            verbs.extend(lemma.name() for lemma in syn.lemmas())\n",
    "        elif syn.pos() == 'a' or syn.pos() == 's':\n",
    "            adjectives.extend(lemma.name() for lemma in syn.lemmas())\n",
    "        elif syn.pos() == 'r':\n",
    "            adverbs.extend(lemma.name() for lemma in syn.lemmas())\n",
    "    \n",
    "    print(f\"\\nWord: {word}\")\n",
    "    print(f\"Nouns: {list(set(nouns))}\")\n",
    "    print(f\"Verbs: {list(set(verbs))}\")\n",
    "    print(f\"Adjectives: {list(set(adjectives))}\")\n",
    "    print(f\"Adverbs: {list(set(adverbs))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F. DEFINITIONS\n",
      "--------------------------------------------------\n",
      "\n",
      "Word: bear\n",
      "Definition: massive plantigrade carnivorous or omnivorous mammals with long shaggy coats and strong claws\n",
      "\n",
      "Word: set\n",
      "Definition: a group of things of the same kind that belong together and are so used\n",
      "\n",
      "Word: square\n",
      "Definition: (geometry) a plane rectangle with four equal sides and four right angles; a four-sided regular polygon\n",
      "\n",
      "Word: lead\n",
      "Definition: an advantage held by a competitor in a race\n",
      "\n",
      "Word: criteria\n",
      "Definition: a basis for comparison; a reference point against which other things can be evaluated\n"
     ]
    }
   ],
   "source": [
    "# f) Get definitions\n",
    "print(\"\\nF. DEFINITIONS\")\n",
    "print(\"-\" * 50)\n",
    "for word in words:\n",
    "    synsets = wn.synsets(word)\n",
    "    print(f\"\\nWord: {word}\")\n",
    "    if synsets:\n",
    "        print(f\"Definition: {synsets[0].definition()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "G. ANTONYMS\n",
      "--------------------------------------------------\n",
      "\n",
      "Word: bear\n",
      "Antonyms: ['bull']\n",
      "\n",
      "Word: set\n",
      "Antonyms: ['rise']\n",
      "\n",
      "Word: square\n",
      "Antonyms: ['round', 'crooked']\n",
      "\n",
      "Word: lead\n",
      "Antonyms: ['follow', 'deficit']\n",
      "\n",
      "Word: criteria\n",
      "Antonyms: []\n"
     ]
    }
   ],
   "source": [
    "# g) Get antonyms\n",
    "print(\"\\nG. ANTONYMS\")\n",
    "print(\"-\" * 50)\n",
    "for word in words:\n",
    "    synsets = wn.synsets(word)\n",
    "    antonyms = []\n",
    "    for syn in synsets:\n",
    "        for lemma in syn.lemmas():\n",
    "            if lemma.antonyms():\n",
    "                antonyms.extend(ant.name() for ant in lemma.antonyms())\n",
    "    print(f\"\\nWord: {word}\")\n",
    "    print(f\"Antonyms: {list(set(antonyms))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LEMMATIZATION AND STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "H. LEMMATIZATION AND STEMMING COMPARISON\n",
      "--------------------------------------------------\n",
      "\n",
      "Word: running\n",
      "Lemmatized: run\n",
      "Porter Stemmer: run\n",
      "Lancaster Stemmer: run\n",
      "Snowball Stemmer: run\n",
      "\n",
      "Word: fishing\n",
      "Lemmatized: fish\n",
      "Porter Stemmer: fish\n",
      "Lancaster Stemmer: fish\n",
      "Snowball Stemmer: fish\n",
      "\n",
      "Word: arguing\n",
      "Lemmatized: argue\n",
      "Porter Stemmer: argu\n",
      "Lancaster Stemmer: argu\n",
      "Snowball Stemmer: argu\n",
      "\n",
      "Word: flying\n",
      "Lemmatized: fly\n",
      "Porter Stemmer: fli\n",
      "Lancaster Stemmer: fly\n",
      "Snowball Stemmer: fli\n",
      "\n",
      "Word: cities\n",
      "Lemmatized: cities\n",
      "Porter Stemmer: citi\n",
      "Lancaster Stemmer: city\n",
      "Snowball Stemmer: citi\n"
     ]
    }
   ],
   "source": [
    "# h) Lemmatization with different stemmers\n",
    "print(\"\\nH. LEMMATIZATION AND STEMMING COMPARISON\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "test_words = ['running', 'fishing', 'arguing', 'flying', 'cities']\n",
    "for word in test_words:\n",
    "    print(f\"\\nWord: {word}\")\n",
    "    print(f\"Lemmatized: {lemmatizer.lemmatize(word, 'v')}\")\n",
    "    print(f\"Porter Stemmer: {porter.stem(word)}\")\n",
    "    print(f\"Lancaster Stemmer: {lancaster.stem(word)}\")\n",
    "    print(f\"Snowball Stemmer: {snowball.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I. STEMMING VS LEMMATIZATION\n",
      "--------------------------------------------------\n",
      "\n",
      "Word: better\n",
      "Lemmatized (verb): better\n",
      "Lemmatized (noun): better\n",
      "Stemmed: better\n",
      "\n",
      "Word: running\n",
      "Lemmatized (verb): run\n",
      "Lemmatized (noun): running\n",
      "Stemmed: run\n",
      "\n",
      "Word: lives\n",
      "Lemmatized (verb): live\n",
      "Lemmatized (noun): life\n",
      "Stemmed: live\n",
      "\n",
      "Word: heavily\n",
      "Lemmatized (verb): heavily\n",
      "Lemmatized (noun): heavily\n",
      "Stemmed: heavili\n"
     ]
    }
   ],
   "source": [
    "# i) Stemming vs Lemmatization\n",
    "print(\"\\nI. STEMMING VS LEMMATIZATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "example_words = ['better', 'running', 'lives', 'heavily']\n",
    "for word in example_words:\n",
    "    print(f\"\\nWord: {word}\")\n",
    "    print(f\"Lemmatized (verb): {lemmatizer.lemmatize(word, 'v')}\")\n",
    "    print(f\"Lemmatized (noun): {lemmatizer.lemmatize(word, 'n')}\")\n",
    "    print(f\"Stemmed: {porter.stem(word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. POS TAGGING AND Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "J. POS TAGGING\n",
      "--------------------------------------------------\n",
      "\n",
      "NLTK POS Tagging:\n",
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n",
      "\n",
      "spaCy POS Tagging:\n",
      "[('The', 'DET'), ('quick', 'ADJ'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('jumps', 'VERB'), ('over', 'ADP'), ('the', 'DET'), ('lazy', 'ADJ'), ('dog', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "# j) POS Tagging\n",
    "print(\"\\nJ. POS TAGGING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# NLTK POS Tagging\n",
    "print(\"\\nNLTK POS Tagging:\")\n",
    "nltk_pos = pos_tag(word_tokenize(text))\n",
    "print(nltk_pos)\n",
    "\n",
    "# spaCy POS Tagging\n",
    "print(\"\\nspaCy POS Tagging:\")\n",
    "doc = nlp(text)\n",
    "spacy_pos = [(token.text, token.pos_) for token in doc]\n",
    "print(spacy_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K. NAMED ENTITY RECOGNITION\n",
      "--------------------------------------------------\n",
      "\n",
      "NLTK NER:\n",
      "(S\n",
      "  (PERSON Mark/NNP)\n",
      "  works/VBZ\n",
      "  at/IN\n",
      "  (ORGANIZATION Google/NNP)\n",
      "  in/IN\n",
      "  (GPE New/NNP York/NNP)\n",
      "  ,/,\n",
      "  and/CC\n",
      "  he/PRP\n",
      "  earned/VBD\n",
      "  his/PRP$\n",
      "  degree/NN\n",
      "  from/IN\n",
      "  (ORGANIZATION MIT/NNP)\n",
      "  in/IN\n",
      "  2020/CD\n",
      "  ./.)\n",
      "\n",
      "spaCy NER:\n",
      "Entity: Mark, Label: PERSON\n",
      "Entity: Google, Label: ORG\n",
      "Entity: New York, Label: GPE\n",
      "Entity: MIT, Label: ORG\n",
      "Entity: 2020, Label: DATE\n"
     ]
    }
   ],
   "source": [
    "# k) Named Entity Recognition\n",
    "print(\"\\nK. NAMED ENTITY RECOGNITION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "ner_text = \"Mark works at Google in New York, and he earned his degree from MIT in 2020.\"\n",
    "\n",
    "# NLTK NER\n",
    "print(\"\\nNLTK NER:\")\n",
    "nltk_ner = ne_chunk(pos_tag(word_tokenize(ner_text)))\n",
    "print(nltk_ner)\n",
    "\n",
    "# spaCy NER\n",
    "print(\"\\nspaCy NER:\")\n",
    "doc = nlp(ner_text)\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. PARSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "L. PARSING\n",
      "--------------------------------------------------\n",
      "\n",
      "Dependency Parsing:\n",
      "The --> det --> cat\n",
      "cat --> nsubj --> sat\n",
      "sat --> ROOT --> sat\n",
      "on --> prep --> sat\n",
      "the --> det --> mat\n",
      "mat --> pobj --> on\n",
      "\n",
      "Constituency Parsing:\n",
      "\n",
      "Note: Stanford CoreNLP server must be running for constituency parsing\n"
     ]
    }
   ],
   "source": [
    "# l) Dependency and Constituency Parsing\n",
    "print(\"\\nL. PARSING\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "parse_text = \"The cat sat on the mat\"\n",
    "\n",
    "# spaCy Dependency Parsing\n",
    "print(\"\\nDependency Parsing:\")\n",
    "doc = nlp(parse_text)\n",
    "for token in doc:\n",
    "    print(f\"{token.text} --> {token.dep_} --> {token.head.text}\")\n",
    "\n",
    "# Stanford CoreNLP Parser\n",
    "try:\n",
    "    parser = CoreNLPParser(url='http://localhost:9000')\n",
    "    print(\"\\nConstituency Parsing:\")\n",
    "    parse = list(parser.parse(parse_text.split()))\n",
    "    for p in parse:\n",
    "        print(p)\n",
    "except:\n",
    "    print(\"\\nNote: Stanford CoreNLP server must be running for constituency parsing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
